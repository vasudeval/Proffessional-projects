{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fc4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from math import log2\n",
    "from tqdm import tqdm_notebook\n",
    "import cv2\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from scipy.stats import truncnorm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d78979",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TRAIN_AT_IMG_SIZE = 32\n",
    "DATASET = 'celeb_dataset'\n",
    "CHECKPOINT_GEN = \"generator.pth\"\n",
    "CHECKPOINT_CRITIC = \"critic.pth\"\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZES = [200,150,100,100,100]\n",
    "CHANNELS_IMG = 1\n",
    "Z_DIM = 512  # should be 512 in original paper\n",
    "IN_CHANNELS = 512  # should be 512 in original paper\n",
    "CRITIC_ITERATIONS = 1\n",
    "LAMBDA_GP = 10\n",
    "PROGRESSIVE_EPOCHS = [30] * len(BATCH_SIZES)\n",
    "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a450c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "#     fake = torch.nn.functional.interpolate(fake, size=[H,W]) #not required --> check\n",
    "#     print(fake.shape, real.shape)\n",
    "\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "    \n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f1dc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 5, stride = 1, padding = 2, gain = 2):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain/(in_channels*(kernel_size**2)))**0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "        \n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.conv(x*self.scale)+self.bias.view(1,self.bias.shape[0],1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf4d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm,self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x/torch.sqrt(torch.mean(x**2, dim = 1, keepdim = True)+self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85a4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, factor):\n",
    "        super(Upsampler, self).__init__()\n",
    "        self.factor = factor\n",
    "        \n",
    "    def forward(self,x):\n",
    "        input_img = x[0]\n",
    "        size_ip = torch.squeeze(x[1][0])\n",
    "        size_tup = [int(size_ip[0]*self.factor),int(size_ip[1]*self.factor)]\n",
    "        return torch.nn.functional.interpolate(input_img, size = size_tup, mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "897efd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm = True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace = True)\n",
    "        self.pn = PixelNorm()\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8d0ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1,1,1/2,1/4,1/32]\n",
    "\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=CHANNELS_IMG):\n",
    "        super(Generator,self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.z_dim = z_dim\n",
    "        self.lin = nn.Linear(in_features = z_dim, out_features = z_dim*4*4)\n",
    "        self.conv1 = WSConv2d(in_channels = z_dim, out_channels = in_channels,kernel_size = 5,\n",
    "                               stride=1, padding=2)\n",
    "        self.conv2 = WSConv2d(in_channels = in_channels, out_channels = in_channels,kernel_size = 5,\n",
    "                               stride=1, padding=2)\n",
    "        \n",
    "        self.pn = PixelNorm()\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.initial2 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "        self.img_channels = img_channels\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size = 1, stride=1, padding=0)\n",
    "        \n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "        nn.ModuleList([]), nn.ModuleList([self.initial_rgb]),)\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        \n",
    "        for i in range(1,5):\n",
    "            self.up_blocks.append(Upsampler(i/4))\n",
    "            \n",
    "            \n",
    "        for i in range(len(factors)-1):\n",
    "            conv_in_c = int(in_channels*factors[i])\n",
    "            conv_out_c = int(in_channels*factors[i+1])\n",
    "            \n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c,conv_out_c))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c,img_channels, kernel_size = 1, stride = 1, padding=0))\n",
    "            \n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha*generated+(1-alpha)*upscaled)\n",
    "\n",
    "\n",
    "    def forward(self,x,alpha, steps):\n",
    "        x1 = x[0]\n",
    "        x2 = x[1]\n",
    "        out = self.pn(x1)\n",
    "        out = self.lin(x1)\n",
    "        out = out.view(-1,self.z_dim,4,4)\n",
    "#         print(out.shape)\n",
    "        out = self.leaky(self.conv1(out))\n",
    "        out = self.leaky(self.conv2(out))\n",
    "        out = self.initial2(out)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upsampled = self.up_blocks[step]([out,x2])\n",
    "            out = self.prog_blocks[step](upsampled)       \n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps-1](upsampled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fec93f",
   "metadata": {},
   "source": [
    "# Discriminator failing with 1 sample or steps >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "530b7b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Looking at the generator\n",
    "# z_dim = 512\n",
    "# in_channels = 512\n",
    "# gen = Generator(z_dim,in_channels=in_channels)\n",
    "# x = torch.randn((3, Z_DIM))\n",
    "# s = torch.ones(3,1)*torch.tensor([32,32])\n",
    "# z = gen([x,s],0.6,4)\n",
    "# for i in range(5):\n",
    "#     z = gen([x,s],alpha=0.6, steps=i)\n",
    "#     plt.imshow(torchvision.utils.make_grid(z, normalize=True).permute(2,1,0))\n",
    "#     plt.title(z.shape[2:])\n",
    "#     plt.show()\n",
    "# # d = Discriminator(512,1)\n",
    "# # p = d(z.detach(),0.5,4)\n",
    "# # p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc6c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels,img_channels=CHANNELS_IMG):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace = True)\n",
    "        self.final_avg = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        for i in range(len(factors)-1,0,-1):\n",
    "            conv_in = int(in_channels*factors[i])\n",
    "            conv_out = int(in_channels*factors[i-1])\n",
    "            \n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels,conv_in, kernel_size=1, stride=1, padding=0))\n",
    "                                    \n",
    "        self.initial_rgb = WSConv2d(img_channels,in_channels,kernel_size=1, stride=1, padding=0)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.final_block = nn.Sequential(WSConv2d(in_channels+1, in_channels, kernel_size=3,stride=1, padding=1),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        WSConv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=1),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        WSConv2d(in_channels,1,kernel_size=1 ,padding=0, stride=1),\n",
    "                                        self.final_avg)\n",
    "\n",
    "    def fade_in(self,alpha, downscaled, out):\n",
    "        return alpha*out+(1-alpha)*downscaled\n",
    "                    \n",
    "    \n",
    "    def minibatch_std(self,x):\n",
    "        batch_statistics = (torch.std(x,dim=0).mean().repeat(x.shape[0],1,x.shape[2],x.shape[3]))\n",
    "        return torch.cat([x,batch_statistics],dim=1)\n",
    "    \n",
    "    def forward(self,x,alpha,steps):\n",
    "        cur_step = len(self.prog_blocks)-steps\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "                                    \n",
    "        if steps ==0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0],-1)\n",
    "                                    \n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step+1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "                                    \n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06312fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_tensorboard(\n",
    "    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n",
    "):\n",
    "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # take out (up to) 8 examples to plot\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "764c827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(critic, gen, loader, dataset, step, originial_shape, alpha, opt_critic, opt_gen,\n",
    "            tensorboard_step, writer, scaler_gen, scaler_critic,epoch):\n",
    "    \n",
    "    loop = tqdm_notebook(loader, leave=False)\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "#         print(cur_batch_size)\n",
    "        shape_ip = torch.tensor([originial_shape[0], originial_shape[1]])\n",
    "        shape_ip = (torch.ones(cur_batch_size,1)*shape_ip).to(DEVICE)\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM).to(DEVICE)\n",
    "#         print(noise.shape)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen([noise,shape_ip], alpha, step)\n",
    "#             print(fake.shape)\n",
    "            critic_real = critic(real, alpha, step)\n",
    "            critic_fake = critic(fake.detach(), alpha, step)\n",
    "            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "                + LAMBDA_GP * gp\n",
    "                + (0.001 * torch.mean(critic_real ** 2))\n",
    "            )\n",
    "\n",
    "        opt_critic.zero_grad()\n",
    "        scaler_critic.scale(loss_critic).backward()\n",
    "        scaler_critic.step(opt_critic)\n",
    "        scaler_critic.update()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = critic(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        # Update alpha and ensure less than 1\n",
    "        alpha += cur_batch_size / (\n",
    "            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen([noise,shape_ip], alpha, step) * 0.5 + 0.5\n",
    "                torchvision.utils.save_image(fixed_fakes,f\"fake_i_{epoch}_{batch_idx}.png\")\n",
    "#                 print(fixed_fakes.shape)\n",
    "            plot_to_tensorboard(\n",
    "                writer,\n",
    "                loss_critic.item(),\n",
    "                loss_gen.item(),\n",
    "                real.detach(),\n",
    "                fixed_fakes.detach(),\n",
    "                tensorboard_step,\n",
    "            )\n",
    "            tensorboard_step += 1\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n",
    "\n",
    "    return tensorboard_step, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c37ed253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(img_size):\n",
    "    image_size1, image_size2 = img_size\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((int(image_size1), int(image_size2))),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "#     batch_size = BATCH_SIZES[step]\n",
    "    batch_size = 100\n",
    "    dataset = datasets.MNIST(root=\"dataset/\", transform=transform, download=True)\n",
    "    dataset, _ = torch.utils.data.random_split(dataset,[30000,30000])\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,)\n",
    "    \n",
    "#         num_workers=NUM_WORKERS,\n",
    "#         pin_memory=True,\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9749b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS).to(DEVICE)\n",
    "    critic = Discriminator(IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "    opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "    \n",
    "    scaler_critic = torch.cuda.amp.GradScaler()\n",
    "    scaler_gen = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    writer = SummaryWriter(f\"logs/gan1\")\n",
    "    \n",
    "    gen.train()\n",
    "    critic.train()\n",
    "    tensorboard_step = 0\n",
    "    \n",
    "\n",
    "#     for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "    num_epochs = 2\n",
    "    alpha = 1e-5\n",
    "    for i in range(1,5):\n",
    "        loader, dataset = get_loader([(START_TRAIN_AT_IMG_SIZE*(i/4)), (START_TRAIN_AT_IMG_SIZE)*(i/4)])\n",
    "        print('Current img size',[(START_TRAIN_AT_IMG_SIZE*(i/4)), (START_TRAIN_AT_IMG_SIZE)*(i/4)])\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tensorboard_step, alpha = train_fn(critic, gen, loader, dataset, i,[32,32],alpha, opt_critic,\n",
    "                                               opt_gen, tensorboard_step, writer, scaler_gen, scaler_critic,epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae24e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current img size [8.0, 8.0]\n",
      "Epoch [1/2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28091/4011446162.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  loop = tqdm_notebook(loader, leave=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current img size [16.0, 16.0]\n",
      "Epoch [1/2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current img size [24.0, 24.0]\n",
      "Epoch [1/2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current img size [32.0, 32.0]\n",
      "Epoch [1/2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "629d6447",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mgen\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(dis,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdis.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(gen,'gen.pth')\n",
    "torch.save(dis,'dis.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75a973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb75d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e39297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
