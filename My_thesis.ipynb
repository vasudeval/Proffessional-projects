{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58700c30",
      "metadata": {
        "id": "58700c30"
      },
      "outputs": [],
      "source": [
        "# CCSL+ WSSL on 2 embeddings.\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import glob\n",
        "import random\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "from torch import optim\n",
        "\n",
        "##################################################################################\n",
        "Nc = 10 # Number of language classes\n",
        "n_epoch = 10 # Number of epochs\n",
        "IP_dim = 39\n",
        "e_dim = 64*2\n",
        "\n",
        "look_back1 = 20\n",
        "look_back2 = 40\n",
        "\n",
        "def lstm_data(f):\n",
        "\n",
        "    df = pd.read_csv(f,encoding='utf-16',usecols=list(range(0,IP_dim)))\n",
        "    dt = df.astype(np.float32)\n",
        "    X=np.array(dt)\n",
        "\n",
        "    Xdata1=[]\n",
        "    Xdata2=[]\n",
        "    Ydata1 =[]\n",
        "\n",
        "    mu = X.mean(axis=0)\n",
        "    std = X.std(axis=0)\n",
        "    np.place(std, std == 0, 1)\n",
        "    X = (X - mu) / std\n",
        "    f1 = os.path.splitext(f)[0]\n",
        "\n",
        "    #print(f1)\n",
        "    clas=f1[42:46]\n",
        "    #print(clas)\n",
        "\n",
        "    if   clas == 'cant':\n",
        "        Y = 0\n",
        "    elif clas == 'indo':\n",
        "        Y = 1\n",
        "    elif clas == 'japa':\n",
        "        Y = 2\n",
        "    elif clas == 'kaza':\n",
        "        Y = 3\n",
        "    elif clas == 'kore':\n",
        "        Y = 4\n",
        "    elif clas == 'mand':\n",
        "        Y = 5\n",
        "    elif clas == 'russ':\n",
        "        Y = 6\n",
        "    elif clas == 'tibe':\n",
        "        Y = 7\n",
        "    elif clas == 'uygh':\n",
        "        Y = 8\n",
        "    elif clas == 'viet':\n",
        "        Y = 9\n",
        "\n",
        "    Y1=np.array([Y])\n",
        "\n",
        "\n",
        "    for i1 in range(0,len(X)-look_back1,1):    #High resolution low context\n",
        "        a=X[i1:(i1+look_back1),:]\n",
        "        Xdata1.append(a)\n",
        "    Xdata1=np.array(Xdata1)\n",
        "\n",
        "    for i2 in range(0,len(X)-look_back2,2):     #Low resolution long context\n",
        "        b=X[i2:(i2+look_back2):3,:]\n",
        "        Xdata2.append(b)\n",
        "    Xdata2=np.array(Xdata2)\n",
        "\n",
        "\n",
        "    Xdata1 = torch.from_numpy(Xdata1).float()\n",
        "    Xdata2 = torch.from_numpy(Xdata2).float()\n",
        "    Ydata1 = torch.from_numpy(Y1).long()\n",
        "\n",
        "    return Xdata1,Xdata2,Ydata1,Y\n",
        "\n",
        "#################################################################################### Modifying e1e2inter2aa\n",
        "class LSTMNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(IP_dim, 400,bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(2*400, 128,bidirectional=True)\n",
        "        self.lstm3 = nn.LSTM(2*128, 64,bidirectional=True)\n",
        "\n",
        "        self.fc_ha=nn.Linear(e_dim,128)\n",
        "        self.fc_1= nn.Linear(128,1)\n",
        "        self.sftmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x, _ = self.lstm3(x)\n",
        "        ht = x[-1]\n",
        "        ht = torch.unsqueeze(ht, 0)\n",
        "        ha = torch.tanh(self.fc_ha(ht))\n",
        "        alpha = self.fc_1(ha)\n",
        "        al = self.sftmax(alpha) # Attention vector\n",
        "\n",
        "        T = list(ht.shape)[1]  #T=time index\n",
        "        batch_size = list(ht.shape)[0]\n",
        "        dim = list(ht.shape)[2]\n",
        "        c = torch.bmm(al.view(batch_size, 1, T),ht.view(batch_size,T,dim))\n",
        "        #print('c size',c.size())\n",
        "        e = torch.squeeze(c,0)\n",
        "        return e\n",
        "\n",
        "class WSSL_Net(nn.Module):\n",
        "    def __init__(self, model1,model2):\n",
        "        super(WSSL_Net, self).__init__()\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "\n",
        "        self.att1=nn.Linear(e_dim,100)\n",
        "        self.att2= nn.Linear(100,1)\n",
        "        self.bsftmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "        self.lang_classifier= nn.Sequential()\n",
        "        self.lang_classifier.add_module('fc1',nn.Linear(e_dim, Nc, bias=False))\n",
        "\n",
        "\n",
        "    def forward(self, x1,x2):\n",
        "        e1 = self.model1(x1)\n",
        "        e2 = self.model2(x2)\n",
        "        ht_e = torch.cat((e1,e2), dim=0)\n",
        "        ht_e = torch.unsqueeze(ht_e, 0)\n",
        "        ha_e = torch.tanh(self.att1(ht_e))\n",
        "        alp = torch.tanh(self.att2(ha_e))\n",
        "        al= self.bsftmax(alp)\n",
        "        Tb = list(ht_e.shape)[1]\n",
        "        batch_size = list(ht_e.shape)[0]\n",
        "        D = list(ht_e.shape)[2]\n",
        "        u_vec = torch.bmm(al.view(batch_size, 1, Tb),ht_e.view(batch_size,Tb,D))\n",
        "        u_vec = torch.squeeze(u_vec,0)\n",
        "\n",
        "        lang_output = self.lang_classifier(u_vec)      # Output layer\n",
        "\n",
        "        return (u_vec,lang_output, e1, e2)\n",
        "###############################################################################################\n",
        "\n",
        "model1 = LSTMNet()\n",
        "model2 = LSTMNet()\n",
        "\n",
        "model1.cuda()\n",
        "model2.cuda()\n",
        "\n",
        "model = WSSL_Net(model1,model2)\n",
        "model.cuda()\n",
        "optimizer = optim.SGD(model.parameters(),lr = 0.01, momentum= 0.9)\n",
        "\n",
        "loss_lang = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "loss_ccsl = torch.nn.CosineSimilarity()\n",
        "loss_wssl = torch.nn.CosineSimilarity()\n",
        "\n",
        "\n",
        "loss_lang.cuda()\n",
        "loss_ccsl.cuda()\n",
        "\n",
        "\n",
        "manual_seed = random.randint(1,10000)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "\n",
        "files_list=[]\n",
        "folders = glob.glob('/wd/users/t20287/tf1/project_m/mfcc_train/*')\n",
        "for folder in folders:\n",
        "    for f in glob.glob(folder+'/*.csv'):\n",
        "        files_list.append(f)\n",
        "\n",
        "l = len(files_list)\n",
        "random.shuffle(files_list)\n",
        "print('Total Training files: ',l)\n",
        "\n",
        "\n",
        "e1_cntrd = {}\n",
        "for o in range(Nc):  #Initialize centroids, it will be over-written after first epoch\n",
        "    e1_cntrd[o] = np.zeros((1,e_dim))\n",
        "\n",
        "'''e2_cntrd = {}\n",
        "for o in range(Nc):  #Initialize centroids, it will be over-written after first epoch\n",
        "    e2_cntrd[o] = np.zeros((1,e_dim))'''\n",
        "\n",
        "e1_tot = {}\n",
        "#e2_tot = {}\n",
        "T_lang = {}\n",
        "for o in range(Nc):\n",
        "    e1_tot[o] = np.zeros((1,e_dim),dtype = np.float32) # Array for storing the total of all e1 embeddings (class-wise)\n",
        "    #e2_tot[o] = np.zeros((1,e_dim),dtype = np.float32)\n",
        "    T_lang[o] = 0  # Initialize total number of samples per language=0, will be incremented in the loop.\n",
        "\n",
        "\n",
        "e1cntrd_gpu = {}\n",
        "#e2cntrd_gpu = {}\n",
        "err_e1 = np.zeros((1,Nc))\n",
        "#err_e2 = np.zeros((1,Nc))\n",
        "sftmx = torch.nn.Softmax(dim=1)\n",
        "\n",
        "i = 0\n",
        "cost = 0.\n",
        "##############################################################################\n",
        "for fn in files_list:\n",
        "    #print(fn)\n",
        "    df = pd.read_csv(fn,encoding='utf-16',usecols=list(range(0,IP_dim)))\n",
        "    data = df.astype(np.float32)\n",
        "    X = np.array(data)\n",
        "    N,D=X.shape\n",
        "\n",
        "    if N>look_back2:\n",
        "        model.zero_grad()\n",
        "\n",
        "        XX1,XX2,YY1,Yint = lstm_data(fn)\n",
        "        XNP=np.array(XX1)\n",
        "        if(np.isnan(np.sum(XNP))):\n",
        "            continue\n",
        "\n",
        "        XNP=np.array(XX2)\n",
        "        if(np.isnan(np.sum(XNP))):\n",
        "            continue\n",
        "\n",
        "        i = i+1\n",
        "        XX1 = np.swapaxes(XX1,0,1)\n",
        "        XX2 = np.swapaxes(XX2,0,1)\n",
        "        X1 = Variable(XX1,requires_grad=False).cuda()\n",
        "        Y1 = Variable(YY1,requires_grad=False).cuda()\n",
        "        X2 = Variable(XX2,requires_grad=False).cuda()\n",
        "\n",
        "\n",
        "        uvec,fl, e1, e2 = model.forward(X1,X2)\n",
        "        err_l = loss_lang(fl,Y1)\n",
        "\n",
        "        T_err = err_l\n",
        "\n",
        "        T_err.backward()\n",
        "        optimizer.step()\n",
        "        cost = cost + T_err.item()\n",
        "\n",
        "        print(\"2ccsl1.py first batch: completed files  \"+str(i)+\"/\"+str(l)+\" Loss= %.3f\"%(cost/i))\n",
        "\n",
        "        if(i%100==0):\n",
        "            break\n",
        "\n",
        "################################################################################\n",
        "\n",
        "print(\" ########################################################################################################  \")\n",
        "\n",
        "batch = 1  # Number of batches completed\n",
        "for e in range(n_epoch):\n",
        "    cost = 0.\n",
        "    random.shuffle(files_list)\n",
        "    i=0  # number of files completed in the epoch\n",
        "    nf = 0 # Number of files completed/ for centroid computation\n",
        "    batch_filelist =[]\n",
        "    print(\"Extracting embeddings for batch: \",batch)\n",
        "\n",
        "    for fn in files_list:\n",
        "        batch_filelist.append(fn)  # List of file names in the mini batch\n",
        "        df = pd.read_csv(fn,encoding='utf-16',usecols=list(range(0,IP_dim)))\n",
        "        data = df.astype(np.float32)\n",
        "        X = np.array(data)\n",
        "        N,D=X.shape\n",
        "\n",
        "        if N>look_back2:\n",
        "            model.zero_grad()\n",
        "\n",
        "            XX1,XX2,YY1,Yint = lstm_data(fn)\n",
        "            XNP=np.array(XX1)\n",
        "            if(np.isnan(np.sum(XNP))):\n",
        "                continue\n",
        "\n",
        "            XNP=np.array(XX2)\n",
        "            if(np.isnan(np.sum(XNP))):\n",
        "                continue\n",
        "\n",
        "            nf =nf + 1\n",
        "            XX1 = np.swapaxes(XX1,0,1)\n",
        "            XX2 = np.swapaxes(XX2,0,1)\n",
        "            X1 = Variable(XX1,requires_grad=False).cuda()\n",
        "            Y1 = Variable(YY1,requires_grad=False).cuda()\n",
        "            X2 = Variable(XX2,requires_grad=False).cuda()\n",
        "\n",
        "            uvec,fl, e1, e2 = model.forward(X1,X2)\n",
        "            e1c = uvec.cpu().detach().numpy() # Copy of e1 in cpu\n",
        "            #e2c = e2.cpu().detach().numpy() # Copy of e2 in cpu\n",
        "\n",
        "            e1_tot[Yint] = e1_tot[Yint] + e1c  # Add e1 according to language id\n",
        "            #e2_tot[Yint] = e2_tot[Yint] + e2c  # Add e2 according to language id\n",
        "            T_lang[Yint] = T_lang[Yint] + 1   # T,e2cotal u-vec in given language id (Yint is true language label)\n",
        "\n",
        "\n",
        "            if(nf%150==0):\n",
        "                for o in range(Nc):\n",
        "                    e1_cntrd[o] = e1_tot[o]/T_lang[o]\n",
        "                    e1cntrd_gpu[o] = torch.from_numpy(e1_cntrd[o]).type(torch.float32).to(\"cuda\") # Send to GPU\n",
        "                    #e2_cntrd[o] = e2_tot[o]/T_lang[o]\n",
        "                    #e2cntrd_gpu[o] = torch.from_numpy(e2_cntrd[o]).type(torch.float32).to(\"cuda\")\n",
        "\n",
        "                print(\"#########  Completed computing the Centroids for this Minibatch. Now training ###################################\")\n",
        "\n",
        "                for fn in batch_filelist:\n",
        "                    df = pd.read_csv(fn,encoding='utf-16',usecols=list(range(0,IP_dim)))\n",
        "                    data = df.astype(np.float32)\n",
        "                    X = np.array(data)\n",
        "                    N,D=X.shape\n",
        "\n",
        "                    if N>look_back2:\n",
        "                        model.zero_grad()\n",
        "\n",
        "                        XX1,XX2,YY1,Yint = lstm_data(fn)\n",
        "                        XNP=np.array(XX1)\n",
        "                        if(np.isnan(np.sum(XNP))):\n",
        "                            continue\n",
        "\n",
        "                        XNP=np.array(XX2)\n",
        "                        if(np.isnan(np.sum(XNP))):\n",
        "                            continue\n",
        "\n",
        "                        i = i+1\n",
        "                        XX1 = np.swapaxes(XX1,0,1)\n",
        "                        XX2 = np.swapaxes(XX2,0,1)\n",
        "                        X1 = Variable(XX1,requires_grad=False).cuda()\n",
        "                        Y1 = Variable(YY1,requires_grad=False).cuda()\n",
        "                        X2 = Variable(XX2,requires_grad=False).cuda()\n",
        "\n",
        "                        uvec,fl, e1, e2 = model.forward(X1,X2)\n",
        "                        err_l = loss_lang(fl,Y1)\n",
        "                        err_wssl = abs(loss_wssl(e1,e2))\n",
        "\n",
        "                        for o in range(Nc):\n",
        "                            err_e1[0,o] = abs(loss_ccsl(uvec,e1cntrd_gpu[o])).cpu().detach().numpy()\n",
        "                            #err_e2[0,o] = abs(loss_ccsl(e2,e2cntrd_gpu[o])).cpu().detach().numpy()  #Compute similarity with all embeddings\n",
        "\n",
        "\n",
        "                        ccsl_e1 = torch.from_numpy(err_e1).type(torch.float32).to(\"cuda\")\n",
        "                        #print(ccsl_e1)\n",
        "                        prob1 = sftmx(ccsl_e1)  ############################### Take softmax on the similarity values\n",
        "                        print(prob1)\n",
        "                        e1_loss = loss_lang(prob1,Y1)\n",
        "\n",
        "                        '''\n",
        "                        ccsl_e2 = torch.from_numpy(err_e2).type(torch.float32).to(\"cuda\")\n",
        "                        #print(ccsl_e2)\n",
        "                        prob2 = sftmx(ccsl_e2)\n",
        "                        print(prob2)\n",
        "                        print(Y1)\n",
        "                        e2_loss = loss_lang(prob2,Y1)'''\n",
        "\n",
        "                        T_err = err_l + 0.2*err_wssl + 0.5*e1_loss #+ 0.75*e2_loss\n",
        "\n",
        "                        T_err.backward()\n",
        "                        optimizer.step()\n",
        "                        cost = cost + T_err.item()\n",
        "\n",
        "                        #erre1 = e1_loss.cpu().detach().numpy() # erre1 is for printing\n",
        "                        #erre2 = e2_loss.cpu().detach().numpy() # erre2 is for printing\n",
        "                        #print(\"e1_loss=\",erre1,\" e2_loss=\",erre2)\n",
        "\n",
        "                        print(\"AP_OLR 2ccsl_wssl1.py:  epoch \"+str(e+1)+\" completed files  \"+str(i)+\"/\"+str(l)+\"                               Loss= %.3f\"%(cost/i))\n",
        "\n",
        "                print(\"Completed training files in batch \",batch,\" Clear the totals for next batch\")\n",
        "                for o in range(Nc):\n",
        "                    e1_tot[o] = np.zeros((1,e_dim),dtype = np.float32)\n",
        "                    #e2_tot[o] = np.zeros((1,e_dim),dtype = np.float32)\n",
        "                    T_lang[o] = 0  # Initialize total number of samples per language=0, will be incremented in the loop.\n",
        "\n",
        "                batch_filelist = [] # Clear batch_filelist\n",
        "                batch = batch+1\n",
        "\n",
        "\n",
        "    path = \"/wd/users/t20287/tf1/project_m/ccsl_wssl1/\"+str(e+1)+\".pth\"\n",
        "    torch.save(model.state_dict(),path)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}